{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjjonathan14/crowdhuman_tensorRT/blob/main/Copy_of_mnsit_2(2)(2)(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sg-yUsfUrlpl",
        "outputId": "18a69fe0-3cfb-404f-dfba-764c05740a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "z5Ig83E-ruMw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !pip install einops\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def batch_plot(img,n, h, w):\n",
        "  fig = plt.figure(figsize=(n, n))\n",
        "  columns = n\n",
        "  rows = n\n",
        "  for i in range(1, columns*rows):\n",
        "\n",
        "      fig.add_subplot(rows, columns, i)\n",
        "      plt.imshow(img[i])\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "V22oCIG5rUNv"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels: int = 1, patch_size: int = 4, emb_size: int = 784, img_size: int = 28):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            # using a conv layer instead of a linear one -> performance gains\n",
        "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.randn(1 ,1, emb_size))\n",
        "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        b, _, _, _ = x.shape\n",
        "\n",
        "        x = self.projection(x)\n",
        "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
        "        # prepend the cls token to the input\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "     \n",
        "        # add position embedding\n",
        "        x += self.positions\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        # fuse the queries, keys and values in one matrix\n",
        "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
        "        # split keys, queries and values in num_heads\n",
        "       \n",
        "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
        "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
        "        # sum up over the last axis\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "\n",
        "        scaling = self.emb_size ** ( 1 /2)\n",
        "        att = F.softmax(energy, dim=-1) / scaling\n",
        "        att = self.att_drop(att)\n",
        "        # sum up over the third axis\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "\n",
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size: int =768, expansion: int = 4, drop_p: float = 0.):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(expansion * emb_size, emb_size),\n",
        "        )\n",
        "\n",
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 emb_size: int = 768,\n",
        "                 drop_p: float = 0.2,\n",
        "                 forward_expansion: int = 4,\n",
        "                 forward_drop_p: float = 0.1,\n",
        "                 ** kwargs):\n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, **kwargs),\n",
        "                nn.Dropout(drop_p)\n",
        "            )),\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                FeedForwardBlock(\n",
        "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )\n",
        "            ))\n",
        "\n",
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth: int = 12, **kwargs):\n",
        "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
        "\n",
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(self, emb_size: int = 768, n_classes: int = 1000):\n",
        "        super().__init__(\n",
        "            Reduce('b n e -> b e', reduction='mean'),\n",
        "            nn.LayerNorm(emb_size),\n",
        "            nn.Linear(emb_size, n_classes))\n",
        "\n",
        "class ViT(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 64,\n",
        "                 patch_size: int = 16,\n",
        "                 emb_size: int = 768,\n",
        "                 img_size: int = 56,\n",
        "                 depth: int = 12,\n",
        "                 n_classes: int = 1000,\n",
        "                 **kwargs):\n",
        "        super().__init__(\n",
        "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
        "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
        "            ClassificationHead(emb_size, n_classes)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "HkRp4VqMr6Uo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               #torchvision.transforms.CenterCrop((24,24)),\n",
        "                                torchvision.transforms.RandomRotation(10),\n",
        "                                torchvision.transforms.RandomAffine(10),\n",
        "                               #torchvision.transforms.Resize((28,28)),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,)),\n",
        "                             ])),\n",
        "  batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                              #torchvision.transforms.CenterCrop((24,24)),\n",
        "                               #torchvision.transforms.RandomRotation(10),\n",
        "                               #torchvision.transforms.RandomAffine(10),\n",
        "                               #torchvision.transforms.Resize((28,28)),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size, shuffle=False)\n",
        "classes = ('0', '1', '2', '3',\n",
        "           '4', '5', '6', '7', '8', '9')\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "ge4gUnJ4Ek6y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# class Hybrid(nn.Module):\n",
        "#     \"\"\"\n",
        "#     block: A sub module\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, inchannels=1, n_classes=10, patch_size=4, img_size=28, heads=4, dropout=0.1, expansion=4,\n",
        "#                  f_drop=0.1, batch_size=1):\n",
        "#         super(Hybrid, self).__init__()\n",
        "#         self.batch_size = batch_size\n",
        "#         self.patch_size = (img_size//patch_size)**2 + 1\n",
        "#         self.img_size = img_size\n",
        "#         self.embedding = img_size ** 2\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "#         self.conv1 = nn.Conv2d(inchannels, 2 * inchannels, kernel_size=5, padding=2, stride=1, dilation=1)\n",
        "#         self.bn1 = nn.BatchNorm2d(2 * inchannels)\n",
        "#         self.conv2 = nn.Conv2d(2 * inchannels, 2 * inchannels, kernel_size=3, padding=1, stride=1, dilation=1)\n",
        "#         self.conv3 = nn.Conv2d(2 * inchannels, 4 * inchannels, kernel_size=1, padding=0, stride=1, dilation=1)\n",
        "#         self.bn2 = nn.BatchNorm2d(4 * inchannels)\n",
        "#         self.conv4 = nn.Conv2d(4 * inchannels, 4, kernel_size=1, padding=0, stride=1, dilation=1)\n",
        "#         self.bn4 = nn.BatchNorm2d(4)\n",
        "#         # reisdual\n",
        "#         self.conv11 = nn.Conv2d(inchannels, 2, kernel_size=3, padding=1, dilation=1, stride=1)\n",
        "#         self.bn11 = nn.BatchNorm2d(2)\n",
        "#         # transformer\n",
        "#         self.patchEmbedding = PatchEmbedding(in_channels=inchannels, patch_size=patch_size, emb_size=self.embedding,\n",
        "#                                              img_size=img_size)\n",
        "#         self.transformerEncoder = TransformerEncoderBlock(emb_size=self.embedding,\n",
        "#                                                           drop_p=dropout,\n",
        "#                                                           forward_expansion=expansion,\n",
        "#                                                           forward_drop_p=f_drop)\n",
        "        \n",
        "#         self.m = nn.Sequential(\n",
        "#                             #nn.Linear(50*28*28, 50*28*28),\n",
        "#                             nn.Unflatten(2, (28, 28))\n",
        "#                           )\n",
        "#         self.unflatten = nn.Unflatten(1, (inchannels, img_size, img_size))\n",
        "#         self.unflatten1 = nn.Unflatten(1, (img_size, img_size))\n",
        "\n",
        "#         self.conv22 = nn.Conv2d(50, 10, kernel_size=3, padding=1, dilation=1, stride=1)\n",
        "\n",
        "\n",
        "#         self.conv33 = nn.Conv2d(50, 8, kernel_size=3, padding=1, dilation=1, stride=1)\n",
        "\n",
        "#         self.conv44 = nn.Conv2d(14, 8, kernel_size=3, padding=1, dilation=1, stride=1)\n",
        "\n",
        "#         self.maxPool = torch.nn.MaxPool2d(2, 2)\n",
        "#         self.dropout = torch.nn.Dropout(0.1, inplace=True)\n",
        "\n",
        "#         self.full = torch.Tensor().cuda(0)\n",
        "#         self.batch = torch.Tensor().cuda(0)\n",
        "\n",
        "#         self.nn1 = torch.nn.Linear(8*196, 4*196)\n",
        "#         self.nn11 = torch.nn.Linear(4*196, 10)\n",
        "#         self.nn2 = torch.nn.Linear(2*196, 10)\n",
        "#         self.nn3 = torch.nn.Linear(2*196, 10)\n",
        "\n",
        "\n",
        "#         self.patchEmbedding1 = PatchEmbedding(in_channels=2*inchannels, patch_size=patch_size, emb_size=self.embedding,\n",
        "#                                              img_size=img_size)\n",
        "#         self.patchEmbedding2 = PatchEmbedding(in_channels=2*inchannels, patch_size=patch_size, emb_size=self.embedding,\n",
        "#                                              img_size=img_size)\n",
        "#         self.patchEmbedding3 = PatchEmbedding(in_channels=4*inchannels, patch_size=patch_size, emb_size=self.embedding,\n",
        "#                                              img_size=img_size)\n",
        "#         self.patchEmbedding4 = PatchEmbedding(in_channels=4*inchannels, patch_size=patch_size, emb_size=self.embedding,\n",
        "#                                              img_size=img_size)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         y = x.clone\n",
        "#         z = x.clone\n",
        "#         x1 = self.bn1(self.conv1(x))\n",
        "#         a1 = self.patchEmbedding1(x1)\n",
        "#         a1 = self.transformerEncoder(a1)\n",
        "#         a1 = self.m(a1)\n",
        "#         a1 = self.conv22(a1)\n",
        "#         x1 = self.bn1(self.conv2(x1))\n",
        "#         a2 = self.patchEmbedding2(x1)\n",
        "#         a2 = self.transformerEncoder(a2)\n",
        "#         a2 = self.m(a2)\n",
        "#         a2 = self.conv22(a2)\n",
        "#         x1 = self.bn2(self.conv3(x1))\n",
        "#         a3 = self.patchEmbedding3(x1)\n",
        "#         a3 = self.transformerEncoder(a3)\n",
        "#         a3 = self.m(a3)\n",
        "#         a3 = self.conv22(a3)\n",
        "#         x1 = self.bn4(self.conv4(x1))\n",
        "#         a4 = self.patchEmbedding4(x1)\n",
        "#         a4 = self.transformerEncoder(a4)\n",
        "#         a4 = self.m(a4)\n",
        "#         a4 = self.conv22(a4)\n",
        " \n",
        "        \n",
        "       \n",
        "#         # residual\n",
        "#         y = self.bn11(self.conv11(x))\n",
        "#         # transformer\n",
        "\n",
        "#         z = self.patchEmbedding(x)\n",
        "#         z = self.transformerEncoder(z)\n",
        "#         z = self.m(z)\n",
        "#         z = self.conv22(z)\n",
        "\n",
        "#         A = torch.cat((z, a1, a2, a3, a4), 1)\n",
        "#         z = self.conv33(A)\n",
        "        \n",
        "#         x = torch.cat((x1, z, y), 1)\n",
        "#         x = self.conv44(x)\n",
        "#         x = self.maxPool(x)\n",
        "#         # print('z',z.shape, 'y', y.shape, 'x1', x1.shape)\n",
        "#         # x_ = torch.cat((x, x1),1)\n",
        "#         # print('x_', x_.shape)\n",
        "#         # x = x1 + x\n",
        "#         # x = self.maxPool(x_)\n",
        "#         # # x = self.maxPool(x)\n",
        "#         # y = self.maxPool(y)\n",
        "#         # z = self.maxPool(z) \n",
        "\n",
        "#         # # x_ = torch.mean(x_, 1)\n",
        "#         # # y_ = torch.mean(y, 1)\n",
        "#         # # z_ = torch.mean(z, 1)\n",
        "\n",
        "        \n",
        "\n",
        "#         x = torch.flatten(x, start_dim=1)\n",
        "#         # y = torch.flatten(y, start_dim=1)\n",
        "#         # z = torch.flatten(z, start_dim=1)\n",
        "#         # #print('x', x.shape, 'y', y.shape, 'z', z.shape)\n",
        "#         x = self.nn11(self.nn1(x))\n",
        "#         # # y = self.nn2(y)\n",
        "#         # # z = self.nn3(z)\n",
        "\n",
        "#         return x,0,0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "p5rT_dRK5GKj"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.adaptive import AdaptiveLogSoftmaxWithLoss\n",
        "\n",
        "class Hybrid(nn.Module):\n",
        "    \"\"\"\n",
        "    block: A sub module\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, inchannels=1, n_classes=10, patch_size=4, img_size=28, heads=4, dropout=0.1, expansion=4,\n",
        "                 f_drop=0.1, batch_size=1):\n",
        "        super(Hybrid, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.patch_size = (img_size//patch_size)**2 + 1\n",
        "        self.img_size = img_size\n",
        "        self.embedding = img_size ** 2\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(inchannels, 2 * inchannels, kernel_size=5, padding=2, stride=1, dilation=1)\n",
        "        self.bn1 = nn.BatchNorm2d(2 * inchannels)\n",
        "        self.conv2 = nn.Conv2d(2 * inchannels, 2 * inchannels, kernel_size=3, padding=1, stride=1, dilation=1)\n",
        "        self.conv3 = nn.Conv2d(2 * inchannels, 4 * inchannels, kernel_size=1, padding=0, stride=1, dilation=1)\n",
        "        self.bn2 = nn.BatchNorm2d(4 * inchannels)\n",
        "        self.conv4 = nn.Conv2d(4 * inchannels, 4, kernel_size=1, padding=0, stride=1, dilation=1)\n",
        "        self.bn4 = nn.BatchNorm2d(4)\n",
        "        # reisdual\n",
        "        self.conv11 = nn.Conv2d(inchannels, 2, kernel_size=3, padding=1, dilation=1, stride=1)\n",
        "        self.bn11 = nn.BatchNorm2d(2)\n",
        "        # transformer\n",
        "        self.patchEmbedding = PatchEmbedding(in_channels=inchannels, patch_size=patch_size, emb_size=self.embedding,\n",
        "                                             img_size=img_size)\n",
        "        self.transformerEncoder = TransformerEncoderBlock(emb_size=self.embedding,\n",
        "                                                          drop_p=dropout,\n",
        "                                                          forward_expansion=expansion,\n",
        "                                                          forward_drop_p=f_drop)\n",
        "        \n",
        "        self.m = nn.Sequential(\n",
        "                            #nn.Linear(50*28*28, 50*28*28),\n",
        "                            nn.Unflatten(2, (28, 28))\n",
        "                          )\n",
        "        self.unflatten = nn.Unflatten(1, (inchannels, img_size, img_size))\n",
        "        self.unflatten1 = nn.Unflatten(1, (img_size, img_size))\n",
        "\n",
        "        self.conv22 = nn.Conv2d(50, 10, kernel_size=3, padding=1, dilation=1, stride=1)\n",
        "\n",
        "\n",
        "        self.conv33 = nn.Conv2d(50, 8, kernel_size=3, padding=1, dilation=1, stride=1)\n",
        "\n",
        "        self.conv44 = nn.Conv2d(16, 8, kernel_size=3, padding=1, dilation=1, stride=1)\n",
        "\n",
        "        self.maxPool = torch.nn.MaxPool2d(2, 2)\n",
        "        self.dropout = torch.nn.Dropout(0.1, inplace=True)\n",
        "\n",
        "        self.full = torch.Tensor().cuda(0)\n",
        "        self.batch = torch.Tensor().cuda(0)\n",
        "\n",
        "        self.nn1 = torch.nn.Linear(8*49, 4*49)\n",
        "        self.nn11 = torch.nn.Linear(4*49, 10)\n",
        "        # self.nn2 = torch.nn.Linear(2*196, 10)\n",
        "        # self.nn3 = torch.nn.Linear(2*196, 10)\n",
        "\n",
        "\n",
        "        # self.patchEmbedding1 = PatchEmbedding(in_channels=2*inchannels, patch_size=patch_size, emb_size=self.embedding,\n",
        "        #                                      img_size=img_size)\n",
        "        # self.patchEmbedding2 = PatchEmbedding(in_channels=2*inchannels, patch_size=patch_size, emb_size=self.embedding,\n",
        "        #                                      img_size=img_size)\n",
        "        # self.patchEmbedding3 = PatchEmbedding(in_channels=4*inchannels, patch_size=patch_size, emb_size=self.embedding,\n",
        "        #                                      img_size=img_size)\n",
        "        # self.patchEmbedding4 = PatchEmbedding(in_channels=4*inchannels, patch_size=patch_size, emb_size=self.embedding,\n",
        "        #                                      img_size=img_size)\n",
        "        \n",
        "        self.conv1_2 = nn.Conv2d(8, 2 * 8, kernel_size=5, padding=2, stride=1, dilation=1)\n",
        "        self.bn1_2 = nn.BatchNorm2d(2 * 8)\n",
        "        self.conv2_2 = nn.Conv2d(2 * 8, 2 * 8, kernel_size=3, padding=1, stride=1, dilation=1)\n",
        "        self.conv3_2 = nn.Conv2d(2 * 8, 4 * 8, kernel_size=1, padding=0, stride=1, dilation=1)\n",
        "        self.bn2_2 = nn.BatchNorm2d(4 * 8)\n",
        "        self.conv4_2 = nn.Conv2d(4 * 8, 16, kernel_size=1, padding=0, stride=1, dilation=1)\n",
        "        self.bn4_2 = nn.BatchNorm2d(16)\n",
        "\n",
        "         # reisdual\n",
        "        self.conv11_2 = nn.Conv2d(8, 8, kernel_size=3, padding=1, dilation=1, stride=1)\n",
        "        self.bn11_2 = nn.BatchNorm2d(8)\n",
        "\n",
        "        # transformer\n",
        "        self.patchEmbedding_2 = PatchEmbedding(in_channels=8, patch_size=2, emb_size=196,\n",
        "                                             img_size=14)\n",
        "        self.transformerEncoder_2 = TransformerEncoderBlock(emb_size=196,\n",
        "                                                          drop_p=dropout,\n",
        "                                                          forward_expansion=expansion,\n",
        "                                                          forward_drop_p=f_drop, num_heads=4)\n",
        "        \n",
        "        self.m_2 = nn.Sequential(\n",
        "                            #nn.Linear(50*28*28, 50*28*28),\n",
        "                            nn.Unflatten(2, (14, 14))\n",
        "                          )\n",
        "        self.conv22_2 = nn.Conv2d(50, 8, kernel_size=3, padding=1, dilation=1, stride=1)\n",
        "        self.conv44_2 = nn.Conv2d(32, 16, kernel_size=3, padding=1, dilation=1, stride=1)\n",
        "        self.conv44_3 = nn.Conv2d(16, 8, kernel_size=3, padding=1, dilation=1, stride=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x.clone\n",
        "        z = x.clone\n",
        "        x1 = self.bn1(self.conv1(x))\n",
        "        # a1 = self.patchEmbedding1(x1)\n",
        "        # a1 = self.transformerEncoder(a1)\n",
        "        # a1 = self.m(a1)\n",
        "        # a1 = self.conv22(a1)\n",
        "        x1 = self.bn1(self.conv2(x1))\n",
        "        # a2 = self.patchEmbedding2(x1)\n",
        "        # a2 = self.transformerEncoder(a2)\n",
        "        # a2 = self.m(a2)\n",
        "        # a2 = self.conv22(a2)\n",
        "        x1 = self.bn2(self.conv3(x1))\n",
        "        # a3 = self.patchEmbedding3(x1)\n",
        "        # a3 = self.transformerEncoder(a3)\n",
        "        # a3 = self.m(a3)\n",
        "        # a3 = self.conv22(a3)\n",
        "        x1 = self.bn4(self.conv4(x1))\n",
        "        # a4 = self.patchEmbedding4(x1)\n",
        "        # a4 = self.transformerEncoder(a4)\n",
        "        # a4 = self.m(a4)\n",
        "        # a4 = self.conv22(a4)\n",
        " \n",
        "        \n",
        "       \n",
        "        # residual\n",
        "        y = self.bn11(self.conv11(x))\n",
        "        # transformer\n",
        "\n",
        "        z = self.patchEmbedding(x)\n",
        "        z = self.transformerEncoder(z)\n",
        "        z = self.m(z)\n",
        "        z = self.conv22(z)\n",
        "      \n",
        "        A = torch.cat((x1, z, y), 1)\n",
        "        A = self.conv44(A)\n",
        "        A = self.maxPool(A)\n",
        "        \n",
        "        #second block\n",
        "\n",
        "        y_1 = self.bn11_2(self.conv11_2(A))\n",
        "\n",
        "        z_1 = self.patchEmbedding_2(A)\n",
        "    \n",
        "        z_1 = self.transformerEncoder_2(z_1)\n",
        "        z_1 = self.m_2(z_1)\n",
        "        z_1 = self.conv22_2(z_1)\n",
        "\n",
        "        A = self.bn1_2(self.conv1_2(A))\n",
        "        # a1 = self.patchEmbedding1(x1)\n",
        "        # a1 = self.transformerEncoder(a1)\n",
        "        # a1 = self.m(a1)\n",
        "        # a1 = self.conv22(a1)\n",
        "        A = self.bn1_2(self.conv2_2(A))\n",
        "        # a2 = self.patchEmbedding2(x1)\n",
        "        # a2 = self.transformerEncoder(a2)\n",
        "        # a2 = self.m(a2)\n",
        "        # a2 = self.conv22(a2)\n",
        "        A = self.bn2_2(self.conv3_2(A))\n",
        "        # a3 = self.patchEmbedding3(x1)\n",
        "        # a3 = self.transformerEncoder(a3)\n",
        "        # a3 = self.m(a3)\n",
        "        # a3 = self.conv22(a3)\n",
        "        A = self.bn4_2(self.conv4_2(A))\n",
        "        # a4 = self.patchEmbedding4(x1)\n",
        "        # a4 = self.transformerEncoder(a4)\n",
        "        # a4 = self.m(a4)\n",
        "        # a4 = self.conv22(a4)'\n",
        "        AA = torch.cat((A, z_1, y_1), 1)\n",
        "\n",
        "        AA = self.conv44_2(AA)\n",
        "        AA = self.conv44_3(AA)\n",
        "        AA = self.maxPool(AA)\n",
        "\n",
        "\n",
        "        AA = torch.flatten(AA, start_dim=1)\n",
        "     \n",
        "        x = self.nn11(self.nn1(AA))\n",
        "\n",
        "\n",
        "        return x,0,0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "wrfBoj7S2Oj7"
      },
      "outputs": [],
      "source": [
        "model = Hybrid()\n",
        "# model.load_state_dict(torch.load('/content/hybrid95.pth'))\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "EbTJC00nZ5O6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# model = Hybrid()\n",
        "model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "criterion1 = nn.HuberLoss()\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t2pfidktl65",
        "outputId": "2e986ac6-460a-491d-d1b1-70d3242b1115"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "768it [00:43, 18.54it/s]"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "start_acc = 0.0\n",
        "for epoch in range(0,1000):  # loop over the dataset multiple times\n",
        "    running_loss_hybrid = 0.0\n",
        "    running_loss_ssl = 0.0\n",
        "\n",
        "    for i, data in tqdm(enumerate(train_loader, 0)):\n",
        "\n",
        "        if i % len(train_loader) == len(train_loader) - 1:    # print every 2000 mini-batches\n",
        "       \n",
        "            if epoch % 1 == 0 :\n",
        "              \n",
        "              correct = 0\n",
        "              total = 0\n",
        "              # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "              with torch.no_grad():\n",
        "                  for data in test_loader:\n",
        "                      images, labels = data\n",
        "                      images, labels = images.cuda(), labels.cuda()\n",
        "                      # calculate outputs by running images through the network\n",
        "                      outputs, y, z = model(images)\n",
        "                      _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                      # the class with the highest energy is what we choose as prediction\n",
        "                      _, predicted = torch.max(outputs.data, 1)\n",
        "                      total += labels.size(0)\n",
        "                      correct += (predicted == labels).sum().item()\n",
        "              if start_acc < 100 *(float(correct) / total):\n",
        "                start_acc = 100 *(float(correct) / total)\n",
        "                torch.save(model.state_dict(), f'hybrid{epoch}.pth')\n",
        "\n",
        "              print(f'epochs {epoch} Hybrid {float(running_loss_hybrid)/(len(train_loader))} SSL loss {float(running_loss_ssl)/(len(train_loader))} Accuracy: {100 *(float(correct) / total)} %')\n",
        "\n",
        "            running_loss_hybrid = 0.0\n",
        "            running_loss_ssl = 0.0\n",
        "\n",
        "\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        X, Y, Z= model(inputs)\n",
        "        loss1 = criterion(X, labels)\n",
        "        loss1.backward(retain_graph = False)\n",
        "        optimizer.step()\n",
        "        running_loss_hybrid += loss1.item()\n",
        "\n",
        "        # inputs, labels = data\n",
        "        # inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        # optimizer.zero_grad()\n",
        "        # X, Y, Z= model(inputs)\n",
        "        #print('z sh',Z.shape, 'label shape', labels.shape)\n",
        "        # loss2 = criterion1(Z, X)\n",
        "        # loss2.backward()\n",
        "        # optimizer.step()\n",
        "        # running_loss_ssl += loss2.item()\n",
        "\n",
        "\n",
        "\n",
        "              \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RzTMdD5cdGt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}